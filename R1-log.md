# 100_Days_Of_ML_Code

## Day 1- 
    Took care of missing data
    Encoded data into categorical data
    Split the dataset to Train set and Test set
    Feature Scaling performed on data set
      
## Day 2-
    Fitting Simple Linear Regression to the Training Set
    Predicted the Test set results
    Visualised the Training set
    Visualised the Test set
    
## Day 3-
    Fitting Multiple Regression
    Backward Elimination with P-values
    Backward Elimination with P-values and Adjusted R Squared
    
## Day 4-
    Fitting Polynomial Linear Regression
    Visualised the Polynomial Regression Results
    Predicted a new result with Linear Regression
    Predicted a new result with Polynomial Regression
    Made a Template for Regression
  
 ## Day 5-
    Fitting Support Vector Regression
    Visualised Support Vector Regression Results
    Predicted result with Support Vector Regression
    Fitting Decision Tree Regression
    Visualised Decision Tree Regression Results
    Predicted result with Decision Tree Regression
    
## Day 6-
    Fitting Random Forest Regression
    Visualised Random Forest Regression Results
    Predicted result with Random Forest Regression
    
## Day 7-
    Revised all the Regression Technique 
    Predicted a new result with Polynomial Regression
    Predicted result with Decision Tree Regression
    Predicted result with Random Forest Regression
    
 ## Day 8-
    Went through R-Squared
    Went through Adjusted R-Squared
    Understood classification method
        - Logistic Regression

## Day 9-
    Fitting Logistic Regression
    Visualised Logistic Regression Results (using matplotlib and mxltend)
    Predicted result with Logistic Regression
    Made Confusion Matrix
    Fitting K-NN
    Visualised K-NN Results (using matplotlib and mxltend)
    Predicted result with K-NN

## Day 10-
    Fitting SVM
    Predicted result with SVM
    Visualised SVM (using matplotlib and mxltend)
    Fitting Kernel-SVM
    Predicted result with Kernel-SVM
    Visualised Kernel-SVM Results (using matplotlib and mxltend)
    Made Confusion Matrix
  
## Day 11-
    Fitting Naive Bayes Classification
    Predicted result with Naive Bayes Classification
    Visualised Naive Bayes Classification Results (using matplotlib and mxltend)
    Fitting Decision Tree Classification
    Predicted result with Decision Tree Classification
    Visualised Decision Tree Classification Results (using matplotlib and mxltend)
    Fitting Random Forest Classification
    Predicted result with Random Forest Classification
    Visualised Random Forest Classification Results (using matplotlib and mxltend)
    Made Confusion Matrix for all the above predictions
    
## Day 12-
    Studied:
        False Positives & False Negatives
        Confusion Matrix
        Accuracy Paradox
        Cap Curve
        Cap Curve Analysis
        
## Day 13-
    Studied:
        Adjusted R-Squared
        Support Vector Regression
        Naive Bayes Classification
        
## Day 14-
    Studied:
        K-Means Clustering
        Hierarchical Clustering
        
## Day 15-
    Used the elbow method to find the optimal number of cluster
    Applied K-Means Clustering to the dataset
    Visualised the Clusters 

## Day 16-
    Used the dendrogram to find the optimal number of clusters
    Fitted and applied Hierarchical Clustering to the dataset
    Visualised the Clusters 
  
## Day 17-
    Imported datasets for Apriori and Eclat Algorithms respectively
    Trained the datasets for Apriori and Eclat Algorithms respectively
    Visualized and fitted the results for Apriori and Eclat Algorithms respectively
    
## Day 18-
    Studied:
        The Multi Armed Bandid Problem
    Implemented (Click through rate optimization problem):
        Imported datasets for Upper Confidence Bound & Thompson Sampling algorithm
        Hard coded the UCB & Thompson Sampling algorithm and implemented it
        Visualized the results using Histogram

## Day 19-
    Importing the dataset for Natural Language Processing
    Cleaned the texts
    Created Bag of Words model
    Splitted the dataset into the Training set and Test set
    Fitted the classifier to the Training set
    Predicted the Test set results (Naive Bayes)
    Made the Confusion Matrix
    
## Day 20-
    Studied:
        About Deep Learning
        Neurons
        The Activation Function
        How Neural Network works and learns
        Gradient Descent
        Stochastic Gradient Descent
        Backpropagation
       
 ## Day 21-
    Studied:
        Threshold Function
        Sigmoid Function
        Rectifier Function
        Hyperbolic Tangent Function
        
 ## Day 22-
     Installed Theano, Tensorflow, Keras
     Encoded the categorical data
     Splitting the dataset into the Training set and Test set
     Feature Scaling
        
 ## Day 23-
    Initialised the Artificial Neural Networks
    Added the Input Layers and the Hidden layers and the Output Layer
    Compiled and fitted the ANN to Training Set
    Made Predictions Evaluating the models
    
 ## Day 24-
    Studied about Convolutional Neural Networks
        - Convolution
        - Max Pooling
        - Flattening
        - Full Connection
        
 ## Day 25-
    Initialised the CNN
    Step 1 - Convolution
    Step 2 - Maxpooling
    Step 3 - Flattening
    Step 4 - Full Connections
    Compiled the CNN
    Fitted the CNN to the images
 
 ## Day 26-
    Trained and predicted results of 10000 image dataset on CNN
    Applied Principal Component Analysis on logistic regression
    Predicted and graphically represented the results
    
 ## Day 27-
    Applied Linear Discriminant Analysis on logistic regression
    Predicted and graphically represented the results
    Applied Kernel-PCA on logistic regression
    Predicted and graphically represented the results
    
 ## Day 28-
    Applied k-Fold Cross Validation
    Applied Grid Search to find the best model and the best parameters
    Fitted XGBoost to the Training set to find optimum predictions
    
 ## Day 29-
    Evaluating the ANN
    Improving the ANN
    Tuning the ANN
    Evaluating the CNN
    Improving the CNN
    Tuning the CNN

 ## Day 30-
    Studied
        - Recurrent Neural Networks
        - Vanishing Gradient Problem
        - Long Short Term Memory
        - LSTM Variants

 ## Day 31-
    Imported and reshaped Training set
    Initialised and compiled the RNN
    Fitted the RNN to the training set
    Visualised the Stock Prediction Results
   
 ## Day 32-
    Evaluated the RNN
    Improved the RNN
    Tuned the RNN
    Studied Self Organizing Maps
    Learnt how to read advanced SOM
    
 ## Day 33 [Fraud Detection] -
    Imported and Feature scaled the dataset for SOM
    Trained the SOM
    Visualized the results
    Found the frauds from the results
    
 ## Day 34
    Implemented a Hybrid Deep Learning Model (SOM & ANN)
    Studied:
    - Boltzmann Machines
    - Energy Based Models
    - Restricted Boltzmann Machines
    - Contrastive Divergence
    - Deep Belief Networks
    - Deep Boltzmann Machines
 
 ## Day 35 [Movie Prediction System (Boltzmann Machines) - part 1]
    Imported the dataset
    Prepared Training set and Test set
    Extracted number of users and movies
    Converted data into an array with users in lines and movies in columns
    Converted the data into Torch tensors
  
 ## Day 36 [Movie Prediction System (Boltzmann Machines) - part 2]
    Created the architecture of the Neural Network
    Trained the Restricted Boltzmann Machines
    Tested the Restricted Boltzmann Machines

 ## Day 37 
    Studied Generative Adversarial Networks
    Set the Hyperparameters and Transformations
    Defined the weights_init function
    Defined and created the generator & discriminator
    Trained the DCGANs and generated images
    
 ## Day 38
    Studied about Auto Encoders
    Sparse Autoencoders
    Denoising Autoencoders
    Contractive Autoencoders
    Stacked Autoencoders
    Deep Autoencoders
    
    
